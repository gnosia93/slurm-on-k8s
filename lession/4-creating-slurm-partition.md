## íŒŒí‹°ì…˜ í• ë‹¹í•˜ê¸° ##

Slurmì—ì„œ íŒŒí‹°ì…˜(Partition)ì€ ì—¬ëŸ¬ ëŒ€ì˜ ì»´í“¨íŒ… ë…¸ë“œë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ë¬¶ì–´ ë†“ì€ 'ìì› ê·¸ë£¹'ì´ì, ì‚¬ìš©ìê°€ ì‘ì—…ì„ ì œì¶œí•˜ëŠ” 'ëŒ€ê¸°ì—´(Queue)'ì´ë‹¤. Slurm ê³µì‹ ë¬¸ì„œ(SchedMD)ì— ë”°ë¥´ë©´ íŒŒí‹°ì…˜ì€ ì‘ì—…ì˜ íŠ¹ì„±ì´ë‚˜ í•˜ë“œì›¨ì–´ ì‚¬ì–‘ì— ë”°ë¼ ì‹œìŠ¤í…œì„ íš¨ìœ¨ì ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ê´€ë¦¬í•˜ëŠ” í•µì‹¬ ë‹¨ìœ„ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ ìš”ì†Œë¡œ êµ¬ì„±ë˜ì–´ì ¸ ìˆë‹¤. 
* ë…¸ë“œ ë¦¬ìŠ¤íŠ¸ (Nodes): í•´ë‹¹ íŒŒí‹°ì…˜ì— í¬í•¨ëœ ì‹¤ì œ ì„œë²„(ì»´í“¨íŒ… ë…¸ë“œ)ë“¤ì˜ ëª©ë¡ìœ¼ë¡œ í•˜ë‚˜ì˜ ë…¸ë“œê°€ ì—¬ëŸ¬ íŒŒí‹°ì…˜ì— ì¤‘ë³µìœ¼ë¡œ ì†í•  ìˆ˜ë„ ìˆë‹¤.
* ì‹œê°„ ì œí•œ (MaxTime): í•œ ì‘ì—…ì´ í•´ë‹¹ íŒŒí‹°ì…˜ì—ì„œ ìµœëŒ€ ëª‡ ì‹œê°„ ë™ì•ˆ ì‹¤í–‰ë  ìˆ˜ ìˆëŠ”ì§€ ì •ì˜í•œë‹¤. (ì˜ˆ: debug íŒŒí‹°ì…˜ì€ 30ë¶„, batch íŒŒí‹°ì…˜ì€ 2ì¼ ë“±)
* ì‚¬ìš©ì ê¶Œí•œ (AllowGroups): íŠ¹ì • íŒŒí‹°ì…˜ì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì‚¬ìš©ì ê·¸ë£¹ì„ ì œí•œí•˜ì—¬ ë³´ì•ˆì´ë‚˜ ìš°ì„ ìˆœìœ„ë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆë‹¤.
* ìš°ì„ ìˆœìœ„ (Priority): ì—¬ëŸ¬ íŒŒí‹°ì…˜ì´ ë™ì¼í•œ ë…¸ë“œë¥¼ ê³µìœ í•  ë•Œ, ì–´ë–¤ íŒŒí‹°ì…˜ì˜ ì‘ì—…ì„ ë¨¼ì € ì‹¤í–‰í• ì§€ ê²°ì •í•œë‹¤. 

### 1. AMEX CPU íŒŒí‹°ì…˜ ìƒì„± ###
ng-amx ë§¤ë‹ˆì§€ë“œ ë…¸ë“œ ê·¸ë£¹ì˜ ë¼ë²¨ì„ í™•ì¸í•œë‹¤.
```
aws eks describe-nodegroup --cluster-name ${CLUSTER_NAME} \
  --nodegroup-name ng-amx --query 'nodegroup.labels' --output text 
```
[ê²°ê³¼]
```
{
    "alpha.eksctl.io/cluster-name": "slurm-on-eks",
    "alpha.eksctl.io/nodegroup-name": "ng-amx",
    "workload-type": "slurm-compute",
    "architecture": "amx-enabled"
}
```
ng-amx ë…¸ë“œ ê·¸ë£¹ì˜ taint ë¥¼ í™•ì¸í•œë‹¤. 
```
aws eks describe-nodegroup --cluster-name ${CLUSTER_NAME} \
  --nodegroup-name ng-amx --query 'nodegroup.taints'
```
[ê²°ê³¼]
```
[
    {
        "key": "workload",
        "value": "slurm",
        "effect": "NO_SCHEDULE"
    }
]
```

ë…¸ë“œê°€ ì´ë¯¸ ìƒì„±ë˜ì–´ ìˆìœ¼ë¯€ë¡œ Slinkyì—ê²Œ "ë™ì ìœ¼ë¡œ ë„ìš°ì§€ ë§ê³ , ì´ ë¼ë²¨ì´ ë¶™ì€ ë…¸ë“œë¥¼ íŒŒí‹°ì…˜ìœ¼ë¡œ ì¨ë¼"ê³  ì•Œë ¤ì¤€ë‹¤. ì´ë•Œ Toleration ë„ í•¨ê»˜ ì„¤ì •í•œë‹¤. 
íŒŒí‹°ì…˜ ì„¤ì •ì— Tolerationì´ í¬í•¨ë˜ëŠ” ì´ìœ ëŠ” "í•´ë‹¹ íŒŒí‹°ì…˜ìœ¼ë¡œ ì œì¶œëœ ëª¨ë“  ì‘ì—…(Pod)ì— ì´ ì¶œì…ì¦ì„ ìë™ìœ¼ë¡œ ë‹¬ì•„ì£¼ê¸° ìœ„í•¨" ì´ë‹¤. 
```
cat <<EOF > amx-nodeset.yaml
# nodesets ì•„ë˜ì— ë°”ë¡œ ì´ë¦„ì„ í‚¤ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤ (ë¦¬ìŠ¤íŠ¸ '-' ì œê±°)
nodesets:
  ns-amx:
    enabled: true
    replicas: 4                # count ëŒ€ì‹  replicasë¥¼ ì‚¬ìš© (Slinky 1.0.1 ê·œê²©)
    updateStrategy:
      type: RollingUpdate
    podSpec:
      nodeSelector:
        workload-type: "slurm-compute"
        architecture: "amx-enabled"
      tolerations:
        - key: "workload"
          operator: "Equal"
          value: "slurm"
          effect: "NoSchedule"
    slurmd:
      image:
        repository: ghcr.io/slinkyproject/slurmd
        tag: 25.11-ubuntu24.04
      resources:
        limits:
          cpu: "30"
          memory: "120Gi"
    # LogFile sidecar configurations.
    logfile:
      image:
        repository: docker.io/library/alpine
        tag: latest
    extraConfMap:
      CPUs: "32"
      Features: "amx"

# partitions í•˜ìœ„ëŠ” ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì„ ìœ ì§€í•˜ë˜, nodes ì´ë¦„ì´ ìœ„ì™€ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•¨
partitions:
  amx:
    enabled: true
    nodesets: 
      - "ns-amx"
    configMap:
      Default: "YES"
      MaxTime: "infinite"
      State: "UP"
EOF
```

helm show values <chart-name> ë¥¼ ì‚¬ìš©í•˜ë©´ ì°¨íŠ¸ê°€ ì œê³µí•˜ëŠ” values ìƒì„¸ ìŠ¤í™ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. values.yaml ì„ ìˆ˜ì •í• ë•Œ ì°¸ê³ í•´ì„œ ì‘ì„±í•´ì•¼ í•œë‹¤. 
```
helm show values oci://ghcr.io/slinkyproject/charts/slurm
```

helm ì°¨íŠ¸ë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤. 
```
helm upgrade --install slurm oci://ghcr.io/slinkyproject/charts/slurm \
  --reuse-values \
  --namespace=slurm -f amx-nodeset.yaml
```

slurm ì˜¤í¼ë ˆì´í„° ë¡œê·¸ì— ì˜¤ë¥˜ê°€ ì—†ëŠ”ì§€ í™•ì¸í•œë‹¤.
```
kubectl logs -n slinky deployment/slurm-operator
```
íŒŒë“œê°€ ëŒ€ìƒ ë…¸ë“œ ê·¸ë£¹ì˜ ë…¸ë“œë“¤ì— ì œëŒ€ë¡œ ìŠ¤ì¼€ì¤„ë§ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•œë‹¤.
```
kubectl get pods -n slurm -l app.kubernetes.io/instance=slurm-worker-ns-amx 
```
[ê²°ê³¼]
```
NAME                    READY   STATUS    RESTARTS   AGE
slurm-worker-ns-amx-0   2/2     Running   0          4m44s
slurm-worker-ns-amx-1   2/2     Running   0          4m44s
slurm-worker-ns-amx-2   2/2     Running   0          4m44s
slurm-worker-ns-amx-3   2/2     Running   0          4m44s
```

slurmctld íŒŒë“œë¡œ ë¡œê·¸ì¸í•˜ì—¬ ì‹ ê·œë¡œ ì¶”ê°€ëœ íŒŒí‹°ì…˜ì„ í™•ì¸í•˜ë‹¤.
```
kubectl exec -it slurm-controller-0 -n slurm -c slurmctld -- /bin/bash
slurm@slurm-controller-0:/tmp$ sinfo
```
[ê²°ê³¼]
```
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
slinky       up   infinite      1   idle slinky-0
all          up   infinite      5   idle ns-amx-[0-3],slinky-0
amx*         up   infinite      4   idle ns-amx-[0-3]
```
amx íŒŒí‹°ì…˜ì˜ ìƒì„¸ ì •ë³´ë¥¼ í™•ì¸í•œë‹¤. 
```
slurm@slurm-controller-0:/tmp$ scontrol show partition amx
```
[ê²°ê³¼]
```
slurm@slurm-controller-0:/tmp$ scontrol show partition amx
PartitionName=amx
   AllowGroups=ALL AllowAccounts=ALL AllowQos=ALL
   AllocNodes=ALL Default=YES QoS=N/A
   DefaultTime=NONE DisableRootJobs=NO ExclusiveUser=NO ExclusiveTopo=NO GraceTime=0 Hidden=NO
   MaxNodes=UNLIMITED MaxTime=UNLIMITED MinNodes=0 LLN=NO MaxCPUsPerNode=UNLIMITED MaxCPUsPerSocket=UNLIMITED
   NodeSets=ns-amx
   Nodes=ns-amx-[0-3]
   PriorityJobFactor=1 PriorityTier=1 RootOnly=NO ReqResv=NO OverSubscribe=NO
   OverTimeLimit=NONE PreemptMode=OFF
   State=UP TotalCPUs=128 TotalNodes=4 SelectTypeParameters=NONE
   JobDefaults=(null)
   DefMemPerNode=UNLIMITED MaxMemPerNode=UNLIMITED
   TRES=cpu=128,mem=507024M,node=4,billing=128
```

### 2. NVIDIA GPU íŒŒí‹°ì…˜ ìƒì„± (Karpenter) ###




SlinkyëŠ” Slurmì˜ ì‘ì—… ìš”ì²­ì„ Kubernetesì˜ Pod ìš”ì²­ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ì´ë•Œ Karpenter(ì¹´íœí„°)ê°€ ì´ Podì„ ë³´ê³  "p4dn 2ëŒ€ê°€ í•„ìš”í•˜ë„¤?"ë¼ë©° AWS EC2ë¥¼ ì¦‰ì‹œ ìƒì„±í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ì— ë¶™ì¸ë‹¤.
sinfoì—ì„œ í™•ì¸í–ˆì„ ë•Œ íŒŒí‹°ì…˜ ìƒíƒœê°€ idle í˜¹ì€ cloudë¡œ ë³´ì¼ ìˆ˜ ìˆëŠ”ë°, ì´ëŠ” ë…¸ë“œê°€ í˜„ì¬ëŠ” ì—†ì§€ë§Œ, ì‘ì—… ì œì¶œ ì‹œ ìë™ìœ¼ë¡œ ìƒì„±ëœë‹¤ëŠ” ëœ»ì´ë‹¤.
GPU íŒŒí‹°ì…˜ ì„¤ì • ì‹œ AWS EFA(Elastic Fabric Adapter) í™œì„±í™” ì˜µì…˜ì´ íŒŒí‹°ì…˜ ì •ì˜ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ê¼­ í™•ì¸í•´ì•¼ í•œë‹¤.


* ì¹´íœí„° ì„¤ì¹˜
* ë…¸ë“œí’€ ì„¤ì •

* 3. Slinkyì™€ì˜ ì—°ê²° (Taint & Toleration)
ì´ê²Œ ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤! Slurm ì‘ì—…ì´ ë“¤ì–´ì™”ì„ ë•Œ ì¹´íœí„°ê°€ "ì•„, ì´ê±´ Slurmìš© ë…¸ë“œêµ¬ë‚˜"ë¼ê³  ì•Œ ìˆ˜ ìˆë„ë¡ Taint(ìš©ì¸) ì„¤ì •ì„ ë§ì¶°ì•¼ í•©ë‹ˆë‹¤.
Slurm íŒŒí‹°ì…˜ ì„¤ì •: Helm values.yamlì˜ partitions ì„¹ì…˜ì— í•´ë‹¹ ë…¸ë“œí’€ì˜ ë ˆì´ë¸”ì´ë‚˜ Taintë¥¼ ê¸°ì…í•©ë‹ˆë‹¤.
ë™ì‘ ì›ë¦¬: sbatch ì œì¶œ â†’ Slinkyê°€ Pod ìƒì„± â†’ Podì— slurm-job ê´€ë ¨ Toleration ë¶€ì—¬ â†’ ì¹´íœí„°ê°€ ì´ë¥¼ ë³´ê³  ì¼ì¹˜í•˜ëŠ” NodePoolì—ì„œ p4dn ì‹¤í–‰.

* 4. ì£¼ì˜ì‚¬í•­ (Scale-down)
Time-to-Live (TTL): ì‘ì—…ì´ ëë‚˜ê³  ë…¸ë“œê°€ ì¦‰ì‹œ ì‚­ì œë˜ê¸¸ ì›í•œë‹¤ë©´ ì¹´íœí„° ì„¤ì •ì—ì„œ disruption.consolidationPolicy: WhenEmptyë¥¼ ì„¤ì •í•˜ì„¸ìš”. Karpenter ì •ì§€ ì„¤ì • ê°€ì´ë“œì—ì„œ ìƒì„¸ ë‚´ìš©ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ê²°ë¡ ì ìœ¼ë¡œ, ì¹´íœí„° ì„¤ì¹˜ + ë…¸ë“œí’€ ì„¤ì • + Slinky íŒŒí‹°ì…˜ ë ˆì´ë¸” ë§¤ì¹­ ì´ 3ë°•ìê°€ ë§ìœ¼ë©´ ìë™ìœ¼ë¡œ p4dnì´ ìƒê²¼ë‹¤ ì‚¬ë¼ì¡Œë‹¤ í•˜ëŠ” ë™ì  í™˜ê²½ì´ ì™„ì„±ë©ë‹ˆë‹¤.
í˜„ì¬ ë…¸ë“œí’€ YAMLì„ ì§ì ‘ ì‘ì„± ì¤‘ì´ì‹ ê°€ìš”? ì•„ë‹ˆë©´ ê¸°ì¡´ì— ì„¤ì¹˜ëœ ì¹´íœí„°ì— p4dnë§Œ ì¶”ê°€í•˜ë ¤ í•˜ì‹œë‚˜ìš”? Spot ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš© ì—¬ë¶€ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë¹„ìš© ìµœì í™” ì˜µì…˜ë„ ë§ë¶™ì—¬ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.


* Slinky í™˜ê²½ì—ì„œ Slurm íŒŒí‹°ì…˜ê³¼ Karpenter ë…¸ë“œí’€ì„ ì—°ê²°í•˜ëŠ” í•µì‹¬ì€ "ì´ íŒŒí‹°ì…˜ì— ì œì¶œëœ ì‘ì—…ì€ ë°˜ë“œì‹œ ì´ ë…¸ë“œ(Karpenterê°€ ë„ìš´ ë…¸ë“œ) ìœ„ì—ì„œë§Œ ì‹¤í–‰ë˜ì–´ì•¼ í•œë‹¤"ëŠ” ì œì•½ ì¡°ê±´ì„ ê±°ëŠ” ê²ƒì…ë‹ˆë‹¤.
* Slinky Helm Chart ê°€ì´ë“œì™€ ì¼ë°˜ì ì¸ Slurm-on-K8s êµ¬ì¡°ì— ë”°ë¥´ë©´, values.yamlì— ì•„ë˜ì™€ ê°™ì´ nodeSelectorì™€ tolerationsë¥¼ ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤.

[values.yaml]
```
clusters:
  - name: "slinky-cluster"
    partitions:
      - name: "gpu-partition"
        instance_types: ["p4dn.24xlarge"]
        # 1. ë…¸ë“œ ì„ íƒ (NodePoolì˜ labelsì™€ ì¼ì¹˜í•´ì•¼ í•¨)
        nodeSelector:
          karpenter.sh/nodepool: slurm-gpu-pool
        
        # 2. í…Œì¸íŠ¸ í—ˆìš© (NodePoolì— ì„¤ì •ëœ taintsê°€ ìˆë‹¤ë©´ í•„ìˆ˜)
        tolerations:
          - key: "slinky.io/usage"
            operator: "Equal"
            value: "gpu-task"
            effect: "NoSchedule"
        
        gres: "gpu:8"

```

```
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: slurm-gpu-pool
spec:
  template:
    spec:
      requirements:
        - key: "node.kubernetes.io/instance-type"
          operator: In
          values: ["p4dn.24xlarge"]
        - key: "karpenter.sh/capacity-type"
          operator: In
          values: ["on-demand"] # ë˜ëŠ” spot
      nodeClassRef:
        name: slurm-gpu-nodeclass
---
apiVersion: karpenter.aws/v1beta1
kind: EC2NodeClass
metadata:
  name: slurm-gpu-nodeclass
spec:
  amiFamily: AL2 # ë˜ëŠ” Bottlerocket
  subnetSelectorTerms:
    - tags: { "karpenter.sh/discovery": "my-cluster" }
  securityGroupSelectorTerms:
    - tags: { "karpenter.sh/discovery": "my-cluster" }
  # p4dnì„ ìœ„í•œ EFA ì„¤ì •ì€ AMI ë‚´ë¶€ì— êµ¬ì„±ë˜ê±°ë‚˜ UserDataë¡œ ì²˜ë¦¬
```


### 3. íŒŒí‹°ì…˜ í™•ì¸í•˜ê¸° ###
```
scontrol show config | grep ClusterName
scontrol show partition gpu-partition
```

ğŸš€ ë‹¤ìŒ ì•¡ì…˜ ì œì•ˆ :

* Slinky í™˜ê²½ì€ Node Selectorë‚˜ Toleration ê°™ì€ ì¿ ë²„ë„¤í‹°ìŠ¤ ê°œë…ì´ Slurm íŒŒí‹°ì…˜ê³¼ ì—°ê²°ë˜ì–´ ì‘ë™í•œë‹¤. 
* í˜¹ì‹œ í˜„ì¬ ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ì„ ì¶”ê°€í•˜ë ¤ í•˜ì‹œë‚˜ìš”, ì•„ë‹ˆë©´ ê¸°ì¡´ íŒŒí‹°ì…˜ì˜ íƒ€ì„ì•„ì›ƒ(Timeout) ì„¤ì •ì„ ë³€ê²½í•˜ë ¤ í•˜ì‹œë‚˜ìš”? 


## GRES / TRES ##
Slurm ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ì˜ í•µì‹¬ì¸ ë‘ ìš©ì–´ëŠ” "ë¬´ì—‡ì„ ê´€ë¦¬í•˜ëŠëƒ"ì™€ "ì–´ë–»ê²Œ ì¹´ìš´íŒ…í•˜ëŠëƒ"ì˜ ì°¨ì´ì…ë‹ˆë‹¤. Slinky(AWS) í™˜ê²½ì—ì„œëŠ” íŠ¹íˆ GPUì™€ ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ í• ë‹¹ì„ ìœ„í•´ ì´ ê°œë…ì„ ì •í™•íˆ ì“°ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. SchedMD GRES ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì •ë¦¬í•´ ë“œë¦½ë‹ˆë‹¤.

### 1. GRES (Generic Resources) ###
CPU/ë©”ëª¨ë¦¬ ì™¸ì— ì‚¬ìš©ìê°€ ìš”ì²­í•˜ëŠ” íŠ¹ìˆ˜ í•˜ë“œì›¨ì–´ë¡œ gpu, mps, fpga ë“±ì„ ì˜ë¯¸í•œë‹¤.
ì‚¬ìš©ìê°€ sbatch --gres=gpu:8ê³¼ ê°™ì´ ìš”ì²­(Request)í•  ë•Œ ì‚¬ìš©ëœë‹¤. 
p4dn.24xlarge ì¸ìŠ¤í„´ìŠ¤ê°€ ìƒì„±ë  ë•Œ "ì´ ë…¸ë“œì—” GPU 8ê°œê°€ ìˆë‹¤"ê³  Slurmì— ì•Œë ¤ì£¼ëŠ” ê¼¬ë¦¬í‘œ ì—­í• ì„ í•œë‹¤.

### 2. TRES (Trackable Resources) ###
Slurmì´ ì¶”ì í•˜ê³  ê¸°ë¡(Accounting)í•  ìˆ˜ ìˆëŠ” ëª¨ë“  ìì›ì„ í†µì¹­í•˜ëŠ” ê²ƒìœ¼ë¡œ GRESë³´ë‹¤ ë” ë„“ì€ ê°œë…ì´ë‹¤.
cpu, mem, node, energy + ëª¨ë“  GRES ê°€ í¬í•¨ë˜ëŠ”ë° ì£¼ë¡œ ê´€ë¦¬ìê°€ ì‚¬ìš©ëŸ‰ ì œí•œ(Quota)ì„ ê±¸ê±°ë‚˜, ë‚˜ì¤‘ì— ì‚¬ìš©ìê°€ ìì›ì„ ì–¼ë§ˆë‚˜ ì¼ëŠ”ì§€ í†µê³„ë¥¼ ë‚¼ ë•Œ ì‚¬ìš©ëœë‹¤.
AWS ë¹„ìš© ìµœì í™”ë¥¼ ìœ„í•´ "íŠ¹ì • ì‚¬ìš©ìê°€ GPU(TRES)ë¥¼ 100ì‹œê°„ ì´ìƒ ì“°ì§€ ëª»í•˜ê²Œ ì œí•œ"í•˜ëŠ” ë“±ì˜ ê³¼ê¸ˆ ë° ê´€ë¦¬ ì •ì±…ì— ì“°ì¼ ìˆ˜ ìˆë‹¤.


## ì°¸ê³  ##
* slurm ì°¨íŠ¸ value í™•ì¸
```
helm show values oci://ghcr.io/slinkyproject/charts/slurm | grep -A 50 "partitions"
```

* ì°¨íŠ¸ ë‚´ë ¤ ë°›ê¸°
```
# 1. ì°¨íŠ¸ íŒŒì¼ì„ í˜„ì¬ ë””ë ‰í† ë¦¬ì— ë‚´ë ¤ë°›ê¸°
helm pull oci://ghcr.io/slinkyproject/charts/slurm --version 1.0.1
# 2. ì••ì¶• í’€ê¸°
tar -zxvf slurm-1.0.1.tgz
# 3. íŒŒì¼ ìœ„ì¹˜ë¡œ ì´ë™í•˜ì—¬ ë‚´ìš© ë³´ê¸°
cat slurm/templates/nodeset/nodeset-cr.yaml
```
