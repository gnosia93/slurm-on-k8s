## 사전 준비 ##

테라폼을 실행한 로컬 PC 에서 아래와 같이 output 정보를 확인한다.  
```
% terraform output
com_x86_dns = "ec2-43-202-5-201.ap-northeast-2.compute.amazonaws.com"
com_x86_vscode = "http://ec2-43-202-5-201.ap-northeast-2.compute.amazonaws.com:9090"
```

com_x86_vscode 서버에 웹으로 접속한 후, 터미널을 열어 kubectl, eksctl, helm 을 설치한다.
별다른 코멘트가 없다면 모든 작업은 com_x86_vscode 웹환경의 터미널에서 수행한다. 
![](https://github.com/gnosia93/training-on-eks/blob/main/chapter/images/code-server.png)

먼저, 터미널의 프롬프트가 x86_64 임을 확인한다.
```
x86_64 $ 
```
 
#### 1. kubectl 설치 #### 
```
ARCH=amd64     
curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.33.3/2025-08-03/bin/linux/$ARCH/kubectl
chmod +x ./kubectl
mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$HOME/bin:$PATH
echo 'export PATH=$HOME/bin:$PATH' >> ~/.bashrc

kubectl version --client
```

#### 2. eksctl 설치 ####
```
ARCH=amd64    
PLATFORM=$(uname -s)_$ARCH
curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"

tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
sudo install -m 0755 /tmp/eksctl /usr/local/bin && rm /tmp/eksctl

eksctl version
```

#### 3. helm 설치 ####
```
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-4
sh get_helm.sh

helm version
``` 

#### 4. k9s 설치 ####
```
ARCH=amd64
if [ "$(uname -m)" != 'aarch64' ]; then
  ARCH="amd64"
fi
echo ${ARCH}" architecture detected .."
curl --silent --location "https://github.com/derailed/k9s/releases/latest/download/k9s_Linux_${ARCH}.tar.gz" | tar xz -C /tmp
sudo mv /tmp/k9s /usr/local/bin/
k9s version
```

#### 5. eks-node-viewer 설치 ####
```
sudo dnf update -y
sudo dnf install golang -y

# 설치 확인 (v1.11 이상 필요)
go version
go install github.com/awslabs/eks-node-viewer/cmd/eks-node-viewer@latest

echo 'export PATH=$PATH:$(go env GOPATH)/bin' >> ~/.bashrc
source ~/.bashrc
```
go 컴파일 과정에서 다소 시간이 소요된다.

#### 6. python 설정 및 pip 설치 ####
```
sudo dnf install -y python-unversioned-command
sudo dnf install -y python3-pip
```

#### 7. anaconda 설치 ####
```
wget https://repo.anaconda.com/archive/Anaconda3-2023.03-1-Linux-x86_64.sh
sh Anaconda3-2023.03-1-Linux-x86_64.sh  
```

## EKS 클러스터 생성하기 ##

### 1. 환경 설정 ###
```
export AWS_REGION=$(aws ec2 describe-availability-zones --query 'AvailabilityZones[0].RegionName' --output text)
export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
export CLUSTER_NAME="slurm-on-eks"
export K8S_VERSION="1.34"
export KARPENTER_VERSION="1.8.1"
export VPC_ID=$(aws ec2 describe-vpcs --filters Name=tag:Name,Values="${CLUSTER_NAME}" --query "Vpcs[].VpcId" --output text)
```

### 2. 서브넷 식별 ###
클러스터의 데이터 플레인(워커노드 들)은 아래의 프라이빗 서브넷에 위치하게 된다. 
```
aws ec2 describe-subnets \
    --filters "Name=tag:Name,Values=SOE-priv-subnet-*" "Name=vpc-id,Values=${VPC_ID}" \
    --query "Subnets[*].{ID:SubnetId, AZ:AvailabilityZone, Name:Tags[?Key=='Name']|[0].Value}" \
    --output table

SUBNET_IDS=$(aws ec2 describe-subnets \
    --filters "Name=tag:Name,Values=SOE-priv-subnet-*" "Name=vpc-id,Values=${VPC_ID}" \
    --query "Subnets[*].{ID:SubnetId, AZ:AvailabilityZone}" \
    --output text)

if [ -z "$SUBNET_IDS" ]; then
    echo "에러: VPC ${VPC_ID} 에 서브넷이 존재하지 않습니다.."
fi

# YAML 형식에 맞게 동적 문자열 생성 (각 ID 뒤에 ": {}" 추가 및 앞쪽 Identation과 줄바꿈)
SUBNET_YAML=""
if [ -f SUBNET_IDS ]; then
    rm SUBNET_IDS
fi
echo "$SUBNET_IDS" | while read -r az subnet_id;
do
    echo "      ${az}: { id: ${subnet_id} }" >> SUBNET_IDS
done
```
[결과]
```
----------------------------------------------------------------------
|                           DescribeSubnets                          |
+-----------------+----------------------------+---------------------+
|       AZ        |            ID              |        Name         |
+-----------------+----------------------------+---------------------+
|  ap-northeast-2b|  subnet-0ac8189839f1dfd67  |  SOE-priv-subnet-2  |
|  ap-northeast-2c|  subnet-032de9fe98da7fdbb  |  SOE-priv-subnet-3  |
|  ap-northeast-2d|  subnet-0fec677af847b90a2  |  SOE-priv-subnet-4  |
|  ap-northeast-2a|  subnet-0c73c17db8ed13d1b  |  SOE-priv-subnet-1  |
+-----------------+----------------------------+---------------------+
```

### 3. 클러스터 생성 ### 
클러스터 생성 완료까지 약 20 ~ 30분 정도의 시간이 소요된다.
```
cat > cluster.yaml <<EOF 
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: "${CLUSTER_NAME}"
  version: "${K8S_VERSION}"
  region: "${AWS_REGION}"

vpc:
  id: "${VPC_ID}"                    
  subnets:
    private:                                 # 프라이빗 서브넷에 데이터플레인 설치
$(cat SUBNET_IDS)

addons:
  - name: vpc-cni
    podIdentityAssociations:
      - serviceAccountName: aws-node
        namespace: kube-system
        permissionPolicyARNs: 
          - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
  - name: eks-pod-identity-agent
  - name: metrics-server
  - name: kube-proxy
  - name: coredns
  - name: aws-ebs-csi-driver                 # loki-ng 용  

managedNodeGroups:                           # 관리형 노드 그룹
  - name: ng-arm
    instanceType: c7g.2xlarge
    minSize: 2
    maxSize: 2
    desiredCapacity: 2
    amiFamily: AmazonLinux2023
    privateNetworking: true                  # 이 노드 그룹이 PRIVATE 서브넷만 사용하도록 지정합니다.
    iam:
      withAddonPolicies:
        ebs: true                     		 # EBS CSI 드라이버가 작동하기 위한 IAM 권한 부여

  - name: ng-amx
    instanceType: m7i.8xlarge
    minSize: 4
    maxSize: 4
    desiredCapacity: 4
    amiFamily: AmazonLinux2023
    privateNetworking: true                  # 이 노드 그룹이 PRIVATE 서브넷만 사용하도록 지정합니다.
    # 노드 선택을 위한 라벨 추가
    labels:
      workload-type: "slurm-compute"
      architecture: "amx-enabled"
    taints:
      - key: "workload"
        value: "slurm"
        effect: "NoSchedule" 				 # Slurm 작업(toleration 보유) 외에는 이 노드에 배포되지 않음
    iam:
      withAddonPolicies:
        ebs: true                     		 # EBS CSI 드라이버가 작동하기 위한 IAM 권한 부여

iam:
  withOIDC: true 

karpenter:
  version: "${KARPENTER_VERSION}"
  createServiceAccount: true 				 # Karpenter용 IAM Role 자동 생성
EOF
```
```
eksctl create cluster -f cluster.yaml
```

[결과]
```
2026-01-17 07:23:35 [ℹ]  eksctl version 0.221.0
2026-01-17 07:23:35 [ℹ]  using region ap-northeast-2
2026-01-17 07:23:35 [✔]  using existing VPC (vpc-0fda854a281c4693e) and subnets (private:map[ap-northeast-2a:{subnet-0c73c17db8ed13d1b ap-northeast-2a 10.0.10.0/24 0 } ap-northeast-2b:{subnet-0ac8189839f1dfd67 ap-northeast-2b 10.0.11.0/24 0 } ap-northeast-2c:{subnet-032de9fe98da7fdbb ap-northeast-2c 10.0.12.0/24 0 } ap-northeast-2d:{subnet-0fec677af847b90a2 ap-northeast-2d 10.0.13.0/24 0 }] public:map[])
2026-01-17 07:23:35 [!]  custom VPC/subnets will be used; if resulting cluster doesn't function as expected, make sure to review the configuration of VPC/subnets
2026-01-17 07:23:35 [ℹ]  nodegroup "ng-arm" will use "" [AmazonLinux2023/1.34]
2026-01-17 07:23:35 [ℹ]  nodegroup "ng-amx" will use "" [AmazonLinux2023/1.34]
2026-01-17 07:23:35 [!]  Auto Mode will be enabled by default in an upcoming release of eksctl. This means managed node groups and managed networking add-ons will no longer be created by default. To maintain current behavior, explicitly set 'autoModeConfig.enabled: false' in your cluster configuration. Learn more: https://eksctl.io/usage/auto-mode/
2026-01-17 07:23:35 [ℹ]  using Kubernetes version 1.34
2026-01-17 07:23:35 [ℹ]  creating EKS cluster "slurm-on-eks" in "ap-northeast-2" region with managed nodes
2026-01-17 07:23:35 [ℹ]  2 nodegroups (ng-amx, ng-arm) were included (based on the include/exclude rules)
2026-01-17 07:23:35 [ℹ]  will create a CloudFormation stack for cluster itself and 2 managed nodegroup stack(s)
2026-01-17 07:23:35 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=ap-northeast-2 --cluster=slurm-on-eks'
2026-01-17 07:23:35 [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "slurm-on-eks" in "ap-northeast-2"
2026-01-17 07:23:35 [ℹ]  CloudWatch logging will not be enabled for cluster "slurm-on-eks" in "ap-northeast-2"
2026-01-17 07:23:35 [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=ap-northeast-2 --cluster=slurm-on-eks'
...
```

EKS 에서 클러스터 시큐리티 그룹은 컨트롤 플레인과 워커노드 사이의 통신을 가능하게 한다. 컨트롤 플레인은 10250 포트를 통해 노드의 큐블렛과 통신하고 워커노드는 443 포트를 이용하여 컨트롤 플레인의 API 서버에 접근을 시도한다. 아래 명령어는 클러스터 시큐리티 그룹에 "karpenter.sh/discovery=${CLUSTER_NAME}" 태크가 존재하는지 확인하는 스크립트이다. 카펜터가 노드를 생성할때, 이와 동일한 태크를 가진 시큐리티 그룹을 찾아 신규 노드에 할당하게 된다. 시큐리티 그룹 검색에 실패하게 되는 경우, EC2 인스턴스는 생성되지만 EKS 클러스터에 조인하지 못한다.  
```
aws ec2 create-tags \
  --resources $(aws eks describe-cluster --name ${CLUSTER_NAME} --query \
					"cluster.resourcesVpcConfig.clusterSecurityGroupId" --output text) \
  --tags Key=karpenter.sh/discovery,Value=${CLUSTER_NAME}
```
또한 쿠버네티스의 서비스 타입을 Load Balancer 변경시 CLB(Classsic Load Balancer)가 생성되는데, 태그가 없는 경우 CLB는 생성되나 해당 서비스의 Pod와 통신이 되지 않는다.  
```
aws ec2 describe-security-groups \
  --group-ids $(aws eks describe-cluster --name ${CLUSTER_NAME} --query \
					"cluster.resourcesVpcConfig.clusterSecurityGroupId" --output text) \
  --query "SecurityGroups[0].Tags" \
  --output table
```
[결과]
```
----------------------------------------------------------------------------------
|                             DescribeSecurityGroups                             |
+-------------------------------------+------------------------------------------+
|                 Key                 |                  Value                   |
+-------------------------------------+------------------------------------------+
|  aws:eks:cluster-name               |  slurm-on-eks                            |
|  karpenter.sh/discovery             |  slurm-on-eks                            |
|  Name                               |  eks-cluster-sg-slurm-on-eks-1010392501  |
|  kubernetes.io/cluster/slurm-on-eks |  owned                                   |
+-------------------------------------+------------------------------------------+
```

### 추가 정책 설정 ###
클러스터 생성이 완료되면 추가 설정이 필요하다. 카펜터 버전 1.8.1(EKS 1.3.4) 에는 아래와 같은 정책 설정이 누락되어 있어 패치가 필요하다. 
패치를 하지 않는 경우 카펜터가 프러비저닝한 노드가 클러스터에 조인되지 않는다. (노드 describe 시 Not Ready 상태)  

* eksctl-training-on-eks-iamservice-role 에 정책 추가(OIDC 정책 누락)
```
POLICY_JSON=$(cat <<EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": "eks:DescribeCluster",
            "Resource": "arn:aws:eks:${AWS_REGION}:${AWS_ACCOUNT_ID}:cluster/${CLUSTER_NAME}"
        },
        {
            "Effect": "Allow",
            "Action": [
                "iam:CreateInstanceProfile",
                "iam:DeleteInstanceProfile",
                "iam:GetInstanceProfile",
                "iam:TagInstanceProfile",
                "iam:AddRoleToInstanceProfile",
                "iam:RemoveRoleFromInstanceProfile",
                "iam:ListInstanceProfiles"
            ],
            "Resource": "*"
        }
    ]
}
EOF
)

aws iam put-role-policy \
    --role-name eksctl-${CLUSTER_NAME}-iamservice-role \
    --policy-name EKS_OIDC_Support_Policy \
    --policy-document "$POLICY_JSON"
```

## eks 워크샵 인스턴스 출력 ##
Managed Node Group 에 속한 인스턴스들이 프라이빗 서비스넷을 사용하고 있는지 확인한다. 
```
# 서브넷 ID와 Name 태그를 매핑하여 인스턴스 정보와 함께 출력
aws ec2 describe-instances \
    --filters "Name=vpc-id,Values=${VPC_ID}" \
    --query 'Reservations[*].Instances[*].{
        InstanceId: InstanceId,
        Name: Tags[?Key==`Name`].Value | [0],
        NodeGroup: Tags[?Key==`eks:nodegroup-name`].Value | [0],
        SubnetId: SubnetId,
        PublicIp: PublicIpAddress
    }' \
    --output table
```
[결과]
```
----------------------------------------------------------------------------------------------------------------
|                                               DescribeInstances                                              |
+---------------------+----------------------------+------------+-----------------+----------------------------+
|     InstanceId      |           Name             | NodeGroup  |    PublicIp     |         SubnetId           |
+---------------------+----------------------------+------------+-----------------+----------------------------+
|  i-018bfcac187415b75|  slurm-on-eks-ng-amx-Node  |  ng-amx    |  None           |  subnet-0fec677af847b90a2  |
|  i-0b5c09314b2446319|  slurm-on-eks-ng-amx-Node  |  ng-amx    |  None           |  subnet-032de9fe98da7fdbb  |
|  i-0690679b2c478ac9e|  slurm-on-eks-ng-arm-Node  |  ng-arm    |  None           |  subnet-032de9fe98da7fdbb  |
|  i-046f7dd9596828ab6|  slurm-on-eks-ng-amx-Node  |  ng-amx    |  None           |  subnet-0ac8189839f1dfd67  |
|  i-0c143e7e23965a7ec|  slurm-on-eks-ng-arm-Node  |  ng-arm    |  None           |  subnet-0c73c17db8ed13d1b  |
|  i-03cb5c3be3a30f188|  slinky-code-server-x86    |  None      |  13.124.230.121 |  subnet-01ffa6d1d3aebd474  |
|  i-0341e690e5aca7893|  slurm-on-eks-ng-amx-Node  |  ng-amx    |  None           |  subnet-0c73c17db8ed13d1b  |
+---------------------+----------------------------+------------+-----------------+----------------------------+
```
ng-amx 에 생성된 4대의 ec2 인스턴스 및 ng-arm 에 생성된 2대의 인스턴스가 프라이빗 서브넷에 해당된 것을 확인할 수 있다.

## 클러스터 삭제 ##
#### 1. 카펜터 인스턴스 프로파일 삭제 #### 
```
ROLE_NAME="eksctl-KarpenterNodeRole-${CLUSTER_NAME}"
for p in $(aws iam list-attached-role-policies --role-name "$ROLE_NAME" --query 'AttachedPolicies[*].PolicyArn' --output text); do aws iam detach-role-policy --role-name "$ROLE_NAME" --policy-arn "$p"; done
for p in $(aws iam list-role-policies --role-name "$ROLE_NAME" --query 'PolicyNames[*]' --output text); do aws iam delete-role-policy --role-name "$ROLE_NAME" --policy-name "$p"; done
for i in $(aws iam list-instance-profiles-for-role --role-name "$ROLE_NAME" --query 'InstanceProfiles[*].InstanceProfileName' --output text); do aws iam remove-role-from-instance-profile --instance-profile-name "$i" --role-name "$ROLE_NAME"; aws iam delete-instance-profile --instance-profile-name "$i"; done
aws iam delete-role --role-name "$ROLE_NAME"
```

#### 2. 클러스터 삭제 ####
```
eksctl delete cluster -f cluster.yaml
```

## 레퍼런스 ##
* https://docs.aws.amazon.com/ko_kr/ec2/latest/instancetypes/ec2-instance-regions.html


